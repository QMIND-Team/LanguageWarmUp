{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Warmup Full Model\n",
    "\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Cleaning Tools Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the dataset, we will be using Pandas, NumPy, RegEx, NLTK and an autocorrect library. There are some custom solutions here that will be explained further via inline comments. Here, we are just importing the libraries and even selecting specific modules from some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# 'spell' will be our spell checker and corrector for cleaning purposes\n",
    "from autocorrect import spell\n",
    "\n",
    "# A stopword for our purposes is a word that doesn't add a lot of insight to the sentiment of a sentence\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# A lemmatizer is a way for us to find the root of a word. \n",
    "# Using this, 'grows', 'grew', and 'grown' all evaluate to 'grow'.\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# We are removing the word 'not' to avoid a situation where for example 'not good' == 'good' \n",
    "stopword = [word for word in stopword if word != 'not']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines of code make it really easy to manipulate the dataset by organizing it into labelled columns and rows, called a dataframe. We can create functions to manipulate specific columns, and for our purposes, we'd like to play around with the reviews only. To do that, first we must read in the data. We can use Pandas' **pd.read_csv** as seen below to read a specific file, **Yelp.txt**. We just have to indicate that each colummn is separated by a tab, or **\\t** which we have done. **Header** is set to None because we want to read the file from its first line. Finally, encoding has been set to **Latin1**. This is because of a decoding error that was thrown at us. Encoding in another character set seemed to fix the problem and create no new ones, so we didn't look back. \n",
    "\n",
    "Now that the dataset is read in, we can name our columns as they appear from left to right by using the **.columns** method in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpDataset = pd.read_csv('Yelp.txt', sep='\\t', header=None, encoding='latin-1')\n",
    "yelpDataset.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Cleaning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data set up the way it is, our cleaning is made fairly simple. Each of these functions 'reads' each review and then executes its function on each of them. The purpose of each function will be explained with inline comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function takes each review and breaks it up into its individual words\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text) # '\\W' will split on any non-word characters such as a whitespace, comma or period\n",
    "    return tokens\n",
    "\n",
    "# This function removes all characters from 'review' that are NOT alaphabetical (special characters, punctuation etc)\n",
    "def onlyAlpha(tokenizedList):\n",
    "    text = [word for word in tokenizedList if word.isalpha()]\n",
    "    return text\n",
    "\n",
    "# This function removes any instance of a stopword in each review\n",
    "def noStop(tokenizedList):\n",
    "    text = [word for word in tokenizedList if word not in stopword]\n",
    "    return text\n",
    "\n",
    "# This function checks for and corrects spelling errors in each word of each review\n",
    "def spellCheck(tokenizedList):\n",
    "    text = [spell(word) for word in tokenizedList]\n",
    "    return text\n",
    "\n",
    "# This function uses the lemmatizer mentioned earlier on each word of each review\n",
    "def lemmatize(tokenizedList):\n",
    "    text = ' '.join([lm.lemmatize(word) for word in tokenizedList]) # .join is used at the end to return the tokenized\n",
    "    return text                                                     # reviews and returning them to full sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Cleaning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply each function, we decided to use lambda functions. Using lambda functions is beneficial here because we are performing a fairly specific sequence of tasks. On the left of each expression we are naming a new column (usually to reflect a change), and on the right, we are applying a function to each row of our specified column. This leaves us with a new column of data with the function applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport random\\nimport numpy as np\\n\\nwith open(\"FeatureCreate\\\\words.txt\") as f:\\n    words = f.readlines()\\nwords = [x.strip() for x in words]\\n\\nphrases = []\\nfor i in range (0,100):\\n    phraselength = random.randint(5,15)\\n    phrase = []\\n    for j in range(0,phraselength):\\n        choice = random.randint(0,len(words))\\n        phrase.append(words[choice])\\n        sentence = \\' \\'.join(phrase)\\n    phrases.append(sentence)\\n\\n    \\nprint(len(phrases))\\n# REMOVE THIS CELL\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the random junk data\n",
    "# This was just a test for me to figure out how to work it with fake sets of randomly generated sentences\n",
    "# The outputs weren't exactly as expected since the sentences generated didn't follow grammar rules\n",
    "'''\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "Now that the data is clean, in order for our Feature Engineering and Vectorization to proceed we must turn our 2-dimensional DataFrame into two separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Make fake y values (fake sentiments)\\n\\ny_dat = [0] * 100\\nfor i in range(0,50):\\n    y_dat[i] = 1\\n        \\nprint(y_dat)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we can turn our DataFrame column into a 1-dimensional DataFrame \n",
    "review = pd.DataFrame(data = yelpDataset['review_lemmatized'])\n",
    "\n",
    "# Now, we can turn our 1-dimensional DataFrame into a list using Pandas' .tolist method\n",
    "review = review['review_lemmatized'].tolist()\n",
    "\n",
    "#The same is done to our sentiment column\n",
    "sentiment = pd.DataFrame(data = yelpDataset['sentiment'])\n",
    "sentiment = sentiment['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data, with 1- and 2- grams\n",
    "\n",
    "#importing a useful function that converts our data set of sentences into a large matrix of 0's and 1's\n",
    "#the matrix has each row representing a sentence and each column representing a word from the data set (no words are repeated)\n",
    "#for each sentence, a 1 is placed in the columns of the words present in the sentence.\n",
    "#If a word isn't in the sentence, a 0 is put in that column.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#placing binary=false would make the matrix count the frequency of a word in the sentence, instead of just marking it's presence\n",
    "#for some reason this wasn't working unless lowercase=false, some problem with the cleaned data I suppose\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, lowercase=False)\n",
    "\n",
    "#This next line could be used instead if we wished to make the program more sophisticated, but larger/slower\n",
    "#Instead of a column for every word, there would also be columns for every set of two words placed next to each other in the data set\n",
    "#Change the 2 to any integer, but makes the matrix exponentially larger\n",
    "\n",
    "#vectorizer = CountVectorizer(binary=True, lowercase=False, ngram_range=(1, 2))\n",
    "\n",
    "#Simply calling this function on our cleaned data set 'phrases'\n",
    "\n",
    "vector = vectorizer.fit_transform(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Change to a numpy array just so it's easier to manipulate\n",
    "\n",
    "data = vector.todense()\n",
    "data = np.asarray(data)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 1741)\n",
      "(200, 1741)\n",
      "(200, 1741)\n"
     ]
    }
   ],
   "source": [
    "# Split into train, test, and validate sets\n",
    "\n",
    "x_train = np.concatenate([data[:300], data[-300:]])\n",
    "y_train = np.concatenate([df2[:300], df2[-300:]])\n",
    "x_val = np.concatenate([data[300:400], data[600:700]])\n",
    "y_val = np.concatenate([df2[300:400], df2[600:700]])\n",
    "x_test = np.concatenate([data[400:600]])\n",
    "y_test = np.concatenate([df2[400:600]])\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Creation \n",
    "\n",
    "This is where we finally make our neural network to process the vectorized data we made above. This process will consist of importing the libraries we need (keras in our instance), selecting the kind of model we wish to work with, creating the nueral network with it number of nodes and layers, selecting optimizer, loss, and metric functions, training, and then finally testing our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
