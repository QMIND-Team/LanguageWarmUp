{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Warmup Full Model\n",
    "\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0                           Wow... Loved this place.          1\n",
      "1                                 Crust is not good.          0\n",
      "2          Not tasty and the texture was just nasty.          0\n",
      "3  Stopped by during the late May bank holiday of...          1\n",
      "4  The selection on the menu was great and so wer...          1\n",
      "[['wow loved this place']\n",
      " ['crust is not good']\n",
      " ['not tasty and the texture was just nasty']\n",
      " ['stopped by during the late may bank holiday off rick steve recommendation and loved it']\n",
      " ['the selection on the menu was great and so were the prices']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "yelpDataset = pd.read_csv('Yelp.txt', sep='\\t', header=None, encoding='latin-1')\n",
    "yelpDataset.columns = ['review', 'sentiment']\n",
    "\n",
    "print(yelpDataset[:5])\n",
    "\n",
    "\n",
    "def removePunct(text):\n",
    "    noPunct = ''.join([char for char in text if char not in string.punctuation])\n",
    "    return noPunct\n",
    "\n",
    "yelpDataset['review_clean'] = yelpDataset['review'].apply(lambda x: removePunct(x.lower()))\n",
    "\n",
    "df1 = pd.DataFrame(data = yelpDataset['review_clean'])\n",
    "#numpy array for review \n",
    "df1 = df1.values\n",
    "df2 = pd.DataFrame(data = yelpDataset['sentiment'])\n",
    "#numpy array for sentiment \n",
    "df2 = df2.values\n",
    "\n",
    "print(df1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow loved this place', 'crust is not good', 'not tasty and the texture was just nasty', 'stopped by during the late may bank holiday off rick steve recommendation and loved it', 'the selection on the menu was great and so were the prices']\n",
      "[1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Right now df1 is a 2D array.  (Each phrase is itself a single element, 1D array (Print it out and see the two sets of brackets))\n",
    "# Need to collapse that\n",
    "\n",
    "# Quick and dirty\n",
    "phrases = []\n",
    "for phrase in df1:\n",
    "    phrases.append(phrase[0])\n",
    "    \n",
    "# And same for df1\n",
    "y_dat = []\n",
    "for dat in df2:\n",
    "    y_dat.append(dat[0])\n",
    "    \n",
    "print(phrases[:5])\n",
    "print(y_dat[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random junk data\n",
    "\n",
    "'''\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "with open(\"FeatureCreate\\words.txt\") as f:\n",
    "    words = f.readlines()\n",
    "words = [x.strip() for x in words]\n",
    "\n",
    "phrases = []\n",
    "for i in range (0,100):\n",
    "    phraselength = random.randint(5,15)\n",
    "    phrase = []\n",
    "    for j in range(0,phraselength):\n",
    "        choice = random.randint(0,len(words))\n",
    "        phrase.append(words[choice])\n",
    "        sentence = ' '.join(phrase)\n",
    "    phrases.append(sentence)\n",
    "\n",
    "    \n",
    "print(len(phrases))\n",
    "# REMOVE THIS CELL\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Make fake y values (fake sentiments)\n",
    "\n",
    "y_dat = [0] * 100\n",
    "for i in range(0,50):\n",
    "    y_dat[i] = 1\n",
    "        \n",
    "print(y_dat)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data, with 1- and 2- grams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True, lowercase=False)\n",
    "#vectorizer = CountVectorizer(binary=True, lowercase=False, ngram_range=(1, 2))\n",
    "vector = vectorizer.fit_transform(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Change to a numpy array\n",
    "\n",
    "data = vector.todense()\n",
    "data = np.asarray(data)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 2068)\n",
      "(200, 2068)\n",
      "(200, 2068)\n"
     ]
    }
   ],
   "source": [
    "# Split into train, test, and validate sets\n",
    "\n",
    "x_train = np.concatenate([data[:300], data[-300:]])\n",
    "y_train = np.concatenate([y_dat[:300], y_dat[-300:]])\n",
    "x_val = np.concatenate([data[300:400], data[600:700]])\n",
    "y_val = np.concatenate([y_dat[300:400], y_dat[600:700]])\n",
    "x_test = np.concatenate([data[400:600]])\n",
    "y_test = np.concatenate([y_dat[400:600]])\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Lay out the model\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (x_train.shape[1],)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1,  activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 0s 607us/step - loss: 0.0905 - acc: 0.9933 - val_loss: 0.4726 - val_acc: 0.8000\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 0s 101us/step - loss: 0.0777 - acc: 0.9933 - val_loss: 0.4894 - val_acc: 0.8000\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 0s 90us/step - loss: 0.0665 - acc: 0.9950 - val_loss: 0.4898 - val_acc: 0.8000\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 0s 120us/step - loss: 0.0569 - acc: 0.9967 - val_loss: 0.4983 - val_acc: 0.7950\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 0s 107us/step - loss: 0.0483 - acc: 0.9967 - val_loss: 0.5080 - val_acc: 0.8000\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 0s 100us/step - loss: 0.0411 - acc: 1.0000 - val_loss: 0.5215 - val_acc: 0.8000\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 0s 102us/step - loss: 0.0350 - acc: 1.0000 - val_loss: 0.5450 - val_acc: 0.7900\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 0s 100us/step - loss: 0.0296 - acc: 1.0000 - val_loss: 0.5453 - val_acc: 0.8050\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 0s 114us/step - loss: 0.0249 - acc: 1.0000 - val_loss: 0.5602 - val_acc: 0.8050\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 0s 96us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.5831 - val_acc: 0.8050\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 0s 95us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.6089 - val_acc: 0.7950\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 0s 100us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.6191 - val_acc: 0.8000\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 0s 95us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.6472 - val_acc: 0.7950\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 0s 127us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.6577 - val_acc: 0.7950\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 0s 140us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.6761 - val_acc: 0.8000\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 0s 103us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.7082 - val_acc: 0.7900\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 0s 75us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.7152 - val_acc: 0.8000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 0s 129us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.7444 - val_acc: 0.7900\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 0s 101us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.7631 - val_acc: 0.7950\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 0s 115us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.7813 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Training time!\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                       y_train, \n",
    "                       epochs=20,\n",
    "                       batch_size=64,\n",
    "                       validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 82us/step\n",
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "# How did it do?\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print (\"Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
