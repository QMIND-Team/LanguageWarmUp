{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Warmup Full Model\n",
    "\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Cleaning Tools Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the dataset, we will be using Pandas, NumPy, RegEx, NLTK and an autocorrect library. There are some custom solutions here that will be explained further via inline comments. Here, we are just importing the libraries and even selecting specific modules from some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# 'spell' will be our spell checker and corrector for cleaning purposes\n",
    "from autocorrect import spell\n",
    "\n",
    "# A stopword for our purposes is a word that doesn't add a lot of insight to the sentiment of a sentence\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# A lemmatizer is a way for us to find the root of a word. \n",
    "# Using this, 'grows', 'grew', and 'grown' all evaluate to 'grow'.\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# We are removing the word 'not' to avoid a situation where for example 'not good' == 'good' \n",
    "stopword = [word for word in stopword if word != 'not']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines of code make it really easy to manipulate the dataset by organizing it into labelled columns and rows, called a dataframe. We can create functions to manipulate specific columns, and for our purposes, we'd like to play around with the reviews only. To do that, first we must read in the data. We can use Pandas' **pd.read_csv** as seen below to read a specific file, **Yelp.txt**. We just have to indicate that each colummn is separated by a tab, or **\\t** which we have done. **Header** is set to None because we want to read the file from its first line. Finally, encoding has been set to **Latin1**. This is because of a decoding error that was thrown at us. Encoding in another character set seemed to fix the problem and create no new ones, so we didn't look back. \n",
    "\n",
    "Now that the dataset is read in, we can name our columns as they appear from left to right by using the **.columns** method in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpDataset = pd.read_csv('Yelp.txt', sep='\\t', header=None, encoding='latin-1')\n",
    "yelpDataset.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Cleaning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data set up the way it is, our cleaning is made fairly simple. Each of these functions 'reads' each review and then executes its function on each of them. The purpose of each function will be explained with inline comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function takes each review and breaks it up into its individual words\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text) # '\\W' will split on any non-word characters such as a whitespace, comma or period\n",
    "    return tokens\n",
    "\n",
    "# This function removes all characters from 'review' that are NOT alaphabetical (special characters, punctuation etc)\n",
    "def onlyAlpha(tokenizedList):\n",
    "    text = [word for word in tokenizedList if word.isalpha()]\n",
    "    return text\n",
    "\n",
    "# This function removes any instance of a stopword in each review\n",
    "def noStop(tokenizedList):\n",
    "    text = [word for word in tokenizedList if word not in stopword]\n",
    "    return text\n",
    "\n",
    "# This function checks for and corrects spelling errors in each word of each review\n",
    "def spellCheck(tokenizedList):\n",
    "    text = [spell(word) for word in tokenizedList]\n",
    "    return text\n",
    "\n",
    "# This function uses the lemmatizer mentioned earlier on each word of each review\n",
    "def lemmatize(tokenizedList):\n",
    "    text = ' '.join([lm.lemmatize(word) for word in tokenizedList]) # .join is used at the end to return the tokenized\n",
    "    return text                                                     # reviews and returning them to full sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Cleaning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply each function, we decided to use lambda functions. Using lambda functions is beneficial here because we are performing a fairly specific sequence of tasks. On the left of each expression we are naming a new column (usually to reflect a change), and on the right, we are applying a function to each row of our specified column. This leaves us with a new column of data with the function applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .apply() is a pandas method that applies a function across an index/column in a dataframe\n",
    "yelpDataset['review_tokens'] = yelpDataset['review'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "yelpDataset['review_alpha'] = yelpDataset['review_tokens'].apply(lambda x: onlyAlpha(x))\n",
    "yelpDataset['review_nostops'] = yelpDataset['review_alpha'].apply(lambda x: noStop(x))\n",
    "yelpDataset['review_spellCheck'] = yelpDataset['review_nostops'].apply(lambda x: spellCheck(x))\n",
    "yelpDataset['review_lemmatized'] = yelpDataset['review_spellCheck'].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From DataFrame to Separate Lists\n",
    "\n",
    "Now that the data is clean, in order for our Feature Engineering and Vectorization to proceed we must turn our 2-dimensional DataFrame into two separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we can turn our DataFrame column into a 1-dimensional DataFrame \n",
    "review = pd.DataFrame(data = yelpDataset['review_lemmatized'])\n",
    "\n",
    "# Now, we can turn our 1-dimensional DataFrame into a list using Pandas' .tolist method\n",
    "review = review['review_lemmatized'].tolist()\n",
    "\n",
    "#The same is done to our sentiment column\n",
    "sentiment = pd.DataFrame(data = yelpDataset['sentiment'])\n",
    "sentiment = sentiment['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Vectorization\n",
    "\n",
    "### Vectorize data, with 1- and 2- grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are importing a useful function that converts our data set of sentences into a large matrix of 0's and 1's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix has each row representing a sentence and each column representing a word from the data set (no words are repeated). For each sentence, a 1 is placed in the columns of the words present in the sentence. If a word isn't in the sentence, a 0 is put in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True, lowercase=False)\n",
    "# 'binary=false' would make the matrix count the frequency of a word in the sentence, instead of just marking its presence\n",
    "# for some reason this wasn't working unless lowercase=false, some problem with the cleaned data I suppose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line could be used instead if we wished to make the program more sophisticated, but larger/slower. Instead of a column for every word, there would also be columns for every set of two words placed next to each other in the data set. You can change the 2 to any integer, but it makes the matrix exponentially larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(binary=True, lowercase=False, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can simply call this function on our cleaned dataset 'phrases'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.fit_transform(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Change to a numpy array\n",
    "\n",
    "data = vector.todense()\n",
    "data = np.asarray(data)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 1740)\n",
      "(200, 1740)\n",
      "(200, 1740)\n"
     ]
    }
   ],
   "source": [
    "# Split into train, test, and validate sets\n",
    "\n",
    "x_train = np.concatenate([data[:300], data[-300:]])\n",
    "y_train = np.concatenate([sentiment[:300], sentiment[-300:]])\n",
    "x_val = np.concatenate([data[300:400], data[600:700]])\n",
    "y_val = np.concatenate([sentiment[300:400], sentiment[600:700]])\n",
    "x_test = np.concatenate([data[400:600]])\n",
    "y_test = np.concatenate([sentiment[400:600]])\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation \n",
    "\n",
    "This is where we finally make our neural network to process the vectorized data we made above. This process will consist of importing the libraries we need (keras in our instance), selecting the kind of model we wish to work with, creating the nueral network with it number of nodes and layers, selecting optimizer, loss, and metric functions, training, and then finally testing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/envs/QMIND/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.23) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import the models library from the keras main library to use for our model\n",
    "from keras import models\n",
    "\n",
    "#Import the layers library from the keras main library, this will allow us to create a sequential neural network with sequential layers\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "#Will make our model a sequential model, with sequences of dense layers\n",
    "model = models.Sequential()\n",
    "\n",
    "#Will set the first layer as a dense layer with 16 nodes, will use the activation function relu and fit our layer to the data\n",
    "#Worth noting here that the number of layers and number of nodes are rather arbitrary, from our experience\n",
    "#with this particular model, 1 dense layer with 16 nodes each allowed for maximal accuracy in the test data\n",
    "#Will then output a single node dense layer that will output a continuous probability curve via the sigmoid function activator\n",
    "\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (x_train.shape[1],)))\n",
    "model.add(layers.Dense(1,  activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation\n",
    "\n",
    "Now that we have made our model, we must now choose what we want as our metric, our loss function, and our optimizer functions.\n",
    "\n",
    "Very large degree of freedom here, many different loss functions to use, as well as metrics for model evaluation\n",
    "and optimizer functions.\n",
    "\n",
    "From our experience with this model, was more useful to use the following optimizer, loss, and metric functions,\n",
    "but this can vary depending upon the project you are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop',           #Set our optimizer function as \"rmsprop\"\n",
    "              loss = 'binary_crossentropy',    #Set up our loss function as \"binary cross entropy\"\n",
    "              metrics = ['accuracy'])          #Set up our metric function, will use \"accuracy\" here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Now that we have created our model and defined how we will evaluate our model and how it will optimize loss, will now train it.\n",
    "\n",
    "To keep track of how our model is doing, will want to print out a history of each epoch to see how our model is improving, and if it is overfitting.\n",
    "\n",
    "To train the model, must give our model the x_train and y_train data we set aside earlier, set a number of epochs we wish for the model to go through, the number of data entries per epoch, and (optionally), have it analyze a vlaidation set.\n",
    "\n",
    "Again, worth noting here that the number of epochs and batch size are completely aribtrary and will vary form model to model, from our experience, we wished to maximize accuracy, so a greater number of epochs was desirable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 0s 493us/step - loss: 0.6934 - acc: 0.5033 - val_loss: 0.6784 - val_acc: 0.6800\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6707 - acc: 0.7233 - val_loss: 0.6720 - val_acc: 0.7200\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 0s 83us/step - loss: 0.6517 - acc: 0.8133 - val_loss: 0.6629 - val_acc: 0.7600\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 0s 75us/step - loss: 0.6316 - acc: 0.8850 - val_loss: 0.6535 - val_acc: 0.7850\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 0s 57us/step - loss: 0.6096 - acc: 0.9133 - val_loss: 0.6417 - val_acc: 0.7950\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.5870 - acc: 0.9167 - val_loss: 0.6285 - val_acc: 0.7850\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 0s 74us/step - loss: 0.5637 - acc: 0.9267 - val_loss: 0.6157 - val_acc: 0.7750\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.5401 - acc: 0.9317 - val_loss: 0.6020 - val_acc: 0.7750\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 0s 79us/step - loss: 0.5163 - acc: 0.9367 - val_loss: 0.5879 - val_acc: 0.8000\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.4930 - acc: 0.9400 - val_loss: 0.5758 - val_acc: 0.8000\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.4701 - acc: 0.9383 - val_loss: 0.5643 - val_acc: 0.7950\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 0s 55us/step - loss: 0.4475 - acc: 0.9483 - val_loss: 0.5526 - val_acc: 0.7950\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 0s 81us/step - loss: 0.4257 - acc: 0.9533 - val_loss: 0.5396 - val_acc: 0.8000\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.4048 - acc: 0.9533 - val_loss: 0.5294 - val_acc: 0.7900\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 0s 75us/step - loss: 0.3847 - acc: 0.9550 - val_loss: 0.5195 - val_acc: 0.7950\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.3654 - acc: 0.9617 - val_loss: 0.5099 - val_acc: 0.8000\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.3470 - acc: 0.9617 - val_loss: 0.4996 - val_acc: 0.8000\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 0s 71us/step - loss: 0.3295 - acc: 0.9617 - val_loss: 0.4923 - val_acc: 0.8000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3125 - acc: 0.9650 - val_loss: 0.4836 - val_acc: 0.8150\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2964 - acc: 0.9667 - val_loss: 0.4761 - val_acc: 0.8150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 20)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(x_train,                            #Input the x_train data\n",
    "                       y_train,                         #Input the y_train data (features)\n",
    "                       epochs=20,                       #Set our model to go through 20 epochs\n",
    "                       batch_size=64,                   #Set our epoch batch size to 64 data entires\n",
    "                       validation_data=(x_val, y_val))  #Have our model also go through the validation set, x_val, y_val\n",
    "\n",
    "#To prevent overfitting, will plot the model accuracy against the validation accuracy and see where divergence occurs\n",
    "\n",
    "#Will use the matplotlib.pyplot library to access the necessary plotting functions we will need to plot our output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot the model accuracy by calling history.history['acc']\n",
    "plt.plot(history.history['acc'], label = \"Model Accuracy\")\n",
    "\n",
    "#Add the validation accurayc to our plot by calling history.history['val_acc']\n",
    "plt.plot(history.history['val_acc'], label = \"Validation Accuracy\")\n",
    "\n",
    "#Add a grid to the plot for neatness in analysis\n",
    "plt.grid()\n",
    "\n",
    "#Add a legend to our plot\n",
    "plt.legend()\n",
    "\n",
    "#Adding a label to the x-axis \n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "#Adding a label to the y-axis\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "\n",
    "#Adding a limit to the x-axis range, want to make sure we get the full 0-20 epoch range\n",
    "plt.xlim(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing\n",
    "\n",
    "Now that we have tried to prevent overfitting the data and have trained it, let's see how well it does on our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 44us/step\n",
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "#This step is as simple as calling the evaluate function from the keras models library with our x_test and y_test data sets\n",
    "#as parameters to pass in\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "#We've tested it, now let's print out how accurate our model is on the testing data set\n",
    "print (\"Accuracy:\", results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
