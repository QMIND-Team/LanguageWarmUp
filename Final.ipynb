{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Warmup Full Model\n",
    "\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from autocorrect import spell\n",
    "\n",
    "yelpDataset = pd.read_csv('Yelp.txt', sep='\\t', header=None, encoding='latin-1')\n",
    "yelpDataset.columns = ['review', 'sentiment']\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "stopword = [word for word in stopword if word != 'not']\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "def onlyAlpha(tokenizedList):\n",
    "    text = [word for word in tokenizedList if word.isalpha()]\n",
    "    return text\n",
    "\n",
    "def noStop(tokenizedList):\n",
    "    text = [word for word in tokenizedList if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def spellCheck(tokenizedList):\n",
    "    text = [spell(word) for word in tokenizedList]\n",
    "    return text\n",
    "\n",
    "def lemmatize(tokenizedList):\n",
    "    text = ' '.join([lm.lemmatize(word) for word in tokenizedList])\n",
    "    return text\n",
    "\n",
    "#def posTag(tokenizedList):\n",
    "#    text = nltk.pos_tag(tokenizedList)\n",
    " #   return text\n",
    "\n",
    "\n",
    "\n",
    "yelpDataset['review_tokens'] = yelpDataset['review'].apply(lambda x: tokenize(x.lower()))\n",
    "yelpDataset['review_alpha'] = yelpDataset['review_tokens'].apply(lambda x: onlyAlpha(x))\n",
    "yelpDataset['review_nostops'] = yelpDataset['review_alpha'].apply(lambda x: noStop(x))\n",
    "yelpDataset['review_spellCheck'] = yelpDataset['review_nostops'].apply(lambda x: spellCheck(x))\n",
    "yelpDataset['review_lemmatized'] = yelpDataset['review_spellCheck'].apply(lambda x: lemmatize(x))\n",
    "#yelpDataset['review_posTag'] = yelpDataset['review_lemmatized'].apply(lambda x: posTag(x))\n",
    "\n",
    "df1 = pd.DataFrame(data = yelpDataset['review_lemmatized'])\n",
    "#creates a list that can be vectorized later\n",
    "df1 = df1['review_lemmatized'].tolist()\n",
    "df2 = pd.DataFrame(data = yelpDataset['sentiment'])\n",
    "df2 = df2['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data, with 1- and 2- grams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True, lowercase=False)\n",
    "#vectorizer = CountVectorizer(binary=True, lowercase=False, ngram_range=(1, 2))\n",
    "vector = vectorizer.fit_transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Change to a numpy array\n",
    "\n",
    "data = vector.todense()\n",
    "data = np.asarray(data)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 1742)\n",
      "(200, 1742)\n",
      "(200, 1742)\n"
     ]
    }
   ],
   "source": [
    "# Split into train, test, and validate sets\n",
    "\n",
    "x_train = np.concatenate([data[:300], data[-300:]])\n",
    "y_train = np.concatenate([df2[:300], df2[-300:]])\n",
    "x_val = np.concatenate([data[300:400], data[600:700]])\n",
    "y_val = np.concatenate([df2[300:400], df2[600:700]])\n",
    "x_test = np.concatenate([data[400:600]])\n",
    "y_test = np.concatenate([df2[400:600]])\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lay out the model\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (x_train.shape[1],)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1,  activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 910us/step - loss: 0.6944 - acc: 0.5100 - val_loss: 0.6883 - val_acc: 0.6250\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.6773 - acc: 0.7533 - val_loss: 0.6793 - val_acc: 0.6850\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 0s 83us/step - loss: 0.6591 - acc: 0.8233 - val_loss: 0.6660 - val_acc: 0.7150\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 0s 87us/step - loss: 0.6355 - acc: 0.8750 - val_loss: 0.6476 - val_acc: 0.7350\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 0s 83us/step - loss: 0.6071 - acc: 0.9000 - val_loss: 0.6297 - val_acc: 0.7250\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 0s 91us/step - loss: 0.5758 - acc: 0.9200 - val_loss: 0.6084 - val_acc: 0.7300\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 0s 85us/step - loss: 0.5406 - acc: 0.9250 - val_loss: 0.5861 - val_acc: 0.7400\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 0s 87us/step - loss: 0.5026 - acc: 0.9383 - val_loss: 0.5625 - val_acc: 0.7550\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 0s 82us/step - loss: 0.4636 - acc: 0.9500 - val_loss: 0.5409 - val_acc: 0.7700\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.4262 - acc: 0.9583 - val_loss: 0.5199 - val_acc: 0.7700\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 0s 79us/step - loss: 0.3887 - acc: 0.9567 - val_loss: 0.5014 - val_acc: 0.7750\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3524 - acc: 0.9683 - val_loss: 0.4829 - val_acc: 0.7850\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3178 - acc: 0.9700 - val_loss: 0.4676 - val_acc: 0.7800\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 0s 72us/step - loss: 0.2848 - acc: 0.9717 - val_loss: 0.4547 - val_acc: 0.7800\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 0s 72us/step - loss: 0.2539 - acc: 0.9817 - val_loss: 0.4442 - val_acc: 0.7850\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2249 - acc: 0.9800 - val_loss: 0.4364 - val_acc: 0.7950\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1980 - acc: 0.9817 - val_loss: 0.4289 - val_acc: 0.7900\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.1738 - acc: 0.9833 - val_loss: 0.4232 - val_acc: 0.8000\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1522 - acc: 0.9850 - val_loss: 0.4202 - val_acc: 0.7900\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1332 - acc: 0.9917 - val_loss: 0.4184 - val_acc: 0.7950\n"
     ]
    }
   ],
   "source": [
    "# Training time!\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                       y_train, \n",
    "                       epochs=20,\n",
    "                       batch_size=64,\n",
    "                       validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 89us/step\n",
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "# How did it do?\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print (\"Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
