{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by importing all the necessary libraries we'll need for our model\n",
    "\n",
    "import json as j\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 4694: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f349380c6836>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mjson_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'yelp_academic_dataset_review.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mjoined_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"[\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 4694: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "#Now that we have the libraries imported to our file, can start by reading data from the Yelp Review into an array\n",
    "\n",
    "json_data = None\n",
    "with open ('yelp_academic_dataset_review.json') as data_file:\n",
    "    lines = data_file.readlines()\n",
    "    joined_lines = \"[\" + \",\".join(lines) + \"]\"\n",
    "   \n",
    "    json_data = j.loads(joined_lines)\n",
    "    \n",
    "#Put our data into a dataframe using pandas\n",
    "\n",
    "json_data = pd.DataFrame(json_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now have our data in a dataframe, but we should probably clean it first so we can ensure we're using useful data only\n",
    "\n",
    "stemmer = SnowballStemmer('english') #Stems the words down to their root meaning\n",
    "words = stopwords.word('english')    #Deletes words that are not commonly used throughout our data set\n",
    "\n",
    "data['cleaned'] = data['text'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \"\", x).split() if i not in words]).lower()\n",
    "\n",
    "#Now that we've cleaned our data, let's assign it to our testing and training sets, along with their labels\n",
    "X_train, X_test, y_train, y_test - train_test_split(data['cleaned'], data.stars, test_size = 0.2)\n",
    "\n",
    "#data.stars is our target vector and data['cleaned'] is our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's now take our words and tokenize/vectorize them to make them usable by our model\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer(ngram_range = (1,2), stop_words = 'english', sublinear_tf = True)),\n",
    "                    ('chi', SelectKBest(chi2, k = 10000))\n",
    "                    ('clf', LinearSVC(C = 1.0, penalty = 'l1', max_iter = 3000, dual = False))])\n",
    "\n",
    "#TfidfVectorizer generates a large matrix of our words following the \"Bag of Words Model\"\n",
    "#Also \"rates\" words based upon the frequency with which the words appear in our data set as they will be considered \n",
    "#important\n",
    "#ngram_range = (1,2) looks at each word and each pair of words and how they appear in our data set\n",
    "#stop_words = 'english' will remove any non-englosh words as they will not be relevant to our data set\n",
    "\n",
    "\n",
    "#Will now create a model by fitting it to our training data\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "\n",
    "vectorizer = model.named_steps['vect']\n",
    "chi = model.named_steps['chi']\n",
    "clf = model.named_steps['clf']\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names = [feature_names[i] for i in chi.get_support(indices=True)]\n",
    "feature_names = np.asarray(feature_names)\n",
    "\n",
    "target_names = ['1', '2', '3', '4', '5']\n",
    "print(\"Top 10 keywords per class\")\n",
    "for i, label in enumerate(target_names):\n",
    "    top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "    print(\"%s: %s\" % label, \" \".join(feature_names[top10]))\n",
    "    \n",
    "print(\"Accuracy score: \" + str(model.score(X_test, y_test)))\n",
    "\n",
    "print(model.predict(['that was an awesome place. Great food!']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
